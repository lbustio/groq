{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python version: 3.12.6\n",
    "\n",
    "Primero se deben instalar todos los componentes para que el código funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq\n",
    "%pip install tenacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se configuran los parametros de uso de la API de groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEY para Bustio\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "# API KEY para Vitali\n",
    "#GROQ_API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import groq\n",
    "import tenacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Groq client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<groq.Groq object at 0x0000028D289F5FA0>\n"
     ]
    }
   ],
   "source": [
    "# Create a client\n",
    "client = groq.Client(\n",
    "    #api_key=os.environ.get(\"GROQ_API_KEY\"), # Si se quiere usar la API Key exportada al SO.\n",
    "    api_key =GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Client created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle chat completion\n",
    "@tenacity.retry(wait=tenacity.wait_exponential(min=1, max=10), stop=tenacity.stop_after_attempt(3))\n",
    "def chat_completion(prompts, model):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=prompts,\n",
    "            temperature=1,\n",
    "            max_tokens=4096,\n",
    "            top_p=1,\n",
    "            stream=True\n",
    "        )\n",
    "        return response\n",
    "    except groq.RateLimitError:\n",
    "        print(\"Rate limit exceeded. Please try again later.\")\n",
    "    except groq.AuthenticationError:\n",
    "        print(\"Authentication failed. Please check your API key.\")\n",
    "    except groq.GroqError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Client created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<groq.Stream object at 0x0000028D29BB7920>\n"
     ]
    }
   ],
   "source": [
    "#model = \"llama3-8b-8192\" # Erroneous model for rise exceptions\n",
    "model=\"llama-3.1-70b-versatile\"\n",
    "system_prompt = \"You are a weather advisor. Answer all the questions.\"\n",
    "user_prompt = \"Hello, what's the weather like today?\"\n",
    "\n",
    "messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': system_prompt,\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': user_prompt,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "response = chat_completion(messages, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, I need a bit more information to give you an accurate forecast. Can you please tell me your location or the city you're interested in? That way, I can give you the current weather conditions, as well as any forecast updates for the day.\n",
      "\n",
      "Also, please note that my knowledge may not be up to the minute, as my data cutoff is December 2023. But I'll do my best to provide you with a general idea of what you can expect."
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting relevant kwywords/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"llama-3.1-70b-versatile\"\n",
    "\n",
    "system_prompt = \"From the user text I need you to extract the relevant words.\"\n",
    "user_prompt = \"Hillary Clinton agrees with John McCain by voting to give George Bush the benefit of the doubt on Iran.\"\n",
    "\n",
    "messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': system_prompt,\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': user_prompt,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "response = chat_completion(messages, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relevant words extracted from the text are:\n",
      "\n",
      "1. Hillary Clinton\n",
      "2. John McCain\n",
      "3. George Bush\n",
      "4. Iran"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting languages in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"llama-3.1-70b-versatile\"\n",
    "\n",
    "system_prompt = \"From the given text, detect all the languages used and identify the majority language. \\\n",
    "    For example, for the text 'You are linda, ma chérie', the languages are: English, Spanish, French. \\\n",
    "        Your response should be only the majority language followed by a list of all detected languages \\\n",
    "            in the format: ['en', 'spa', 'fr']. Do not provide any additional explanation.\"\n",
    "\n",
    "\n",
    "#user_prompt = \"Hillary Clinton agrees with John McCain by voting to give George Bush the benefit of the doubt on Iran.\"\n",
    "user_prompt = \"Living la vida loca ma chere amie et ego sum vir molto bene!\"\n",
    "\n",
    "messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': system_prompt,\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': user_prompt,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "response = chat_completion(messages, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish,['es', 'fr', 'la']"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
